# Tutorials and Examples

This document provides a collection of tutorials and examples to help you get started with various topics. Each section includes step-by-step instructions and code snippets to guide you through the process.

## The Basics

`margarine` has several different density estimators that can be used to learn probability distributions from samples. Each estimator has a common interface and set of methods including `train()`, `sample()`, `log_prob()`, `log_like()`, `save()` and `load()`. The following example demonstrates how to train a RealNVP density estimator and generate samples from it. Similar patterns can be followed for other estimators in the library and the API reference can be consulted for specifics.

We first need to generate some example samples to train on. In this case we will generate samples from a 2D Gaussian distribution with some correlation between the two dimensions.

```python
import jax
import jax.numpy as jnp
import matplotlib.pyplot as plt

from margarine.estimators.realnvp import RealNVP

nsamples = 5000
key = jax.random.PRNGKey(0)
original_samples = jax.random.multivariate_normal(
    key,
    mean=jnp.array([0.0, 0.0]),
    cov=jnp.array([[1.0, 0.8], [0.8, 1.0]]),
    shape=(nsamples,),
)
```

We can then create a RealNVP density estimator, train it on the samples, and generate new samples from the learned distribution.

```python
realnvp_estimator = RealNVP(
        original_samples,
        in_size=2,
        hidden_size=50,
        num_layers=6,
        num_coupling_layers=6,
    )
key, subkey = jax.random.split(key)
realnvp_estimator.train(
            subkey,
            learning_rate=1e-3,
            epochs=2000,
            patience=50,
            batch_size=1000,
)
generated_samples = realnvp_estimator.sample(subkey, num_samples=nsamples)
```

Finally, we can visualize the original samples and the samples generated by the RealNVP estimator to see how well it has learned the distribution.

```python
plt.scatter(
    original_samples[:, 0], original_samples[:, 1], alpha=0.5, label="Original Samples"
)
plt.scatter(
    generated_samples[:, 0], generated_samples[:, 1], alpha=0.5, label="Generated Samples"
)
```

You can also calculate the log-probability of samples under the learned distribution using the `log_prob()` method.

```python
log_probs = realnvp_estimator.log_prob(original_samples)
print("Log probabilities of original samples:", log_probs)
```

and transform samples from a unit hypercube to the learned distribution using `__call__` method.

```python
key, subkey = jax.random.split(key)
unit_samples = jax.random.uniform(subkey, shape=(nsamples, 2))
transformed_samples = realnvp_estimator(unit_samples)


plt.scatter(
    transformed_samples[:, 0], transformed_samples[:, 1], alpha=0.5, label="Transformed Samples"
)

plt.legend()
plt.title("RealNVP: Original vs Generated Samples")
plt.xlabel("X1")
plt.ylabel("X2")
plt.show()
```

![image](https://raw.githubusercontent.com/htjb/margarine/main/docs/example-images/realnvp_example.png)

This is essentially what the `sample()` method does internally by transforming uniform samples to the learned distribution.

This is just a basic example to get you started. Each density estimator in `margarine` has its own specific parameters and options, so be sure to check the documentation for more details on how to use them effectively.


## Calculating Bayesian Statistics

`margarine` has helper functions to calculate Bayesian statistics including the Kullback-Leibler (KL) divergence and the Bayesian model dimensionality between two density estimators. 

These functions require a density estimator for both the posterior and prior distributions. The following example demonstrates how to calculate the KL divergence and Bayesian model dimensionality between two RealNVP estimators trained on different datasets.

We start by generating some example samples for the prior distribution and training a RealNVP estimator. In practice the helper functions will work with any object that has a `log_prob()` method that takes samples (although you might get type hinting errors if you use an object that doesn't inherit from `margarine.base.baseflow.BaseDensityEstimator`).

```python
prior_samples = jax.random.uniform(
    key,
    shape=(nsamples, 2),
    minval=-4.0,
    maxval=4.0,
)

prior_estimator = RealNVP(
        prior_samples,
        in_size=2,
        hidden_size=50,
        num_layers=6,
        num_coupling_layers=6,
    )
key, subkey = jax.random.split(key)
prior_estimator.train(
            subkey,
            learning_rate=1e-3,
            epochs=2000,
            patience=50,
            batch_size=1000,
)
```

We can then caluclate the statistics using the helper functions.

```python
from margarine.statistics import kldivergence, model_dimensionality

kl_div = kldivergence(realnvp_estimator, prior_estimator, transformed_samples)
model_dim = model_dimensionality(realnvp_estimator, prior_estimator, transformed_samples)
print("KL Divergence:", kl_div)
print("Model Dimensionality:", model_dim)
```


You can also calculate the KL divergence and model dimensionality between differnt types of `margarine` density estimators e.g. between a RealNVP and a KDE estimator.


## Saving and Loading Models

