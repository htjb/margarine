{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed7c3806",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "The following set of examples shows the user how to train a Masked Autoregressive Flow (MAF) and an example Kernel Density Estimator (KDE). We further demonstrate how to use `margarine` to estimate the Kullback Leibler divergence and Bayesian Dimensionality with the trained MAF and KDE.\n",
    "\n",
    "We also demonstrate how to use the clustering feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07930f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c7050a4c",
   "metadata": {},
   "source": [
    "In order to demonstrate the applications of the code we need to generate some mock samples and we can visualise the posterior distributions with `anesthetic`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ad87a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anesthetic import MCMCSamples\n",
    "\n",
    "x = np.random.normal(0, 1, 1000)\n",
    "y = np.random.normal(2, 0.5, 1000)\n",
    "\n",
    "data = np.vstack([x, y]).T\n",
    "weights = np.ones(len(data))\n",
    "\n",
    "samples = MCMCSamples(data=data, weights=weights)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7488b9e7",
   "metadata": {},
   "source": [
    "To visualise the posterior we use anesthetic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2693b2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [i for i in range(2)]\n",
    "samples.plot_2d(names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4769ccfd",
   "metadata": {},
   "source": [
    "## Masked Autoregressive Flows\n",
    "\n",
    "Firstly we will look at training a Masked Autoregressive Flow or MAF with `margarine`. To train the MAF we first need to initalise the class with the samples and corresponding weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f713dc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')\n",
    "\n",
    "from margarine.maf import MAF\n",
    "\n",
    "bij = MAF(data, weights)\n",
    "bij.train(10000, early_stop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c908353",
   "metadata": {},
   "source": [
    "We can then generate uniformly weighted samples from the bijector using the following code which technically takes samples on the hypercube and transforms them into samples on the target posterior distribution,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca97fca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = bij(np.random.uniform(0, 1, size=(len(data), data.shape[-1])))\n",
    "\n",
    "maf_samples = MCMCSamples(data=x, weights=np.ones(len(x)))\n",
    "axes = samples.plot_2d(names, alpha=0.5, label='Original')\n",
    "maf_samples.plot_2d(axes, alpha=0.5, label='MAF')\n",
    "axes.iloc[0, 0].legend()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "384493f8",
   "metadata": {},
   "source": [
    "Alternatively we can generate samples with the following code which takes in an integer and returns an array of shape (int, 5). The `.sample()` function is a proxy for `__call__`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da77e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = bij.sample(5000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0bcd4ae9",
   "metadata": {},
   "source": [
    "We can then go ahead an calculate the corresponding kl divergence and Bayesian dimensionality. \n",
    "\n",
    "The samples presented here were generated using a gaussian likelihood and fitting with nested sampling for 5 parameters. We can use `anesthetic` to calculate the KL divergence and Bayesian dimensionality for the samples for comparison. We see very similar results and note that the similarity improves with the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c268549",
   "metadata": {},
   "outputs": [],
   "source": [
    "from margarine.marginal_stats import calculate\n",
    "\n",
    "stats = calculate(bij).statistics()\n",
    "print(stats.iloc[0, 0], samples.D())\n",
    "print(stats.iloc[1, 0], samples.d())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3de6120",
   "metadata": {},
   "source": [
    "# Kernel Density Estimators\n",
    "\n",
    "We can perform a similar analysis using Kernel Density Estimators rather than MAFs which is done with the following code. Note that the generation of the 'trained' model is significantly quicker than when performed with the MAFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c307af09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from margarine.kde import KDE\n",
    "kde = KDE(data, weights)\n",
    "kde.generate_kde()\n",
    "x = kde.sample(5000)\n",
    "\n",
    "samples = MCMCSamples(data=x, weights=weights)\n",
    "samples.plot_2d(names)\n",
    "\n",
    "stats = calculate(kde).statistics()\n",
    "print(stats.iloc[0, 0], '+(-)', stats.iloc[0, 2] - stats.iloc[0, 0], \n",
    "      '(', stats.iloc[0, 0] - stats.iloc[0, 1], ')', samples.D())\n",
    "print(stats.iloc[1, 0], '+(-)', stats.iloc[1, 2] - stats.iloc[1, 0], \n",
    "      '(', stats.iloc[1, 0] - stats.iloc[1, 1], ')', samples.d())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6037d4d5",
   "metadata": {},
   "source": [
    "Rather than using the `kde.sample()` function to generate samples we could transform samples from the hypercube with the following code and the `__call__()` function. However, we note that this is a much slower method of generating samples as it is designed to be bijective. Transformation from the hypercube is useful if we would like to use a trained KDE or MAF as the prior in a subseqeunt nested sampling run however is not necessary if we simply want to calcualte marginal Bayesian statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30d6f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = kde(np.random.uniform(0, 1, size=(10, data.shape[-1])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7403fd3b",
   "metadata": {},
   "source": [
    "# Clustering with margarine\n",
    "\n",
    "The below example demonstrates how to perform clustering with `margarine` which can improve the accuracy of the emulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e9e10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.concatenate([np.random.normal(0, 1, 500), np.random.normal(5, 0.3, 500)])\n",
    "y = np.random.normal(0, 3, 1000)\n",
    "\n",
    "data = np.vstack([x, y]).T\n",
    "\n",
    "samples = MCMCSamples(data=data, weights=np.ones(len(data)))\n",
    "samples.plot_2d(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed64bf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow = MAF(data, np.ones(len(data)), clustering=True)\n",
    "flow.train(10000, early_stop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd800486",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_samples = flow.sample(5000)\n",
    "flow_samples = MCMCSamples(data=flow_samples, weights=np.ones(len(flow_samples)))\n",
    "flow_samples.plot_2d(names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
